{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, num_layers):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.decoder = nn.Sequential()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.encoder.add_module('encoder_layer{}'.format(i), nn.Linear(input_size, hidden_size))\n",
    "                self.encoder.add_module('encoder_relu{}'.format(i), nn.ReLU())\n",
    "                self.decoder.add_module('decoder_layer{}'.format(i), nn.Linear(latent_size, hidden_size))\n",
    "                self.decoder.add_module('decoder_relu{}'.format(i), nn.ReLU())\n",
    "            elif i == num_layers - 1:\n",
    "                self.encoder.add_module('encoder_layer{}'.format(i), nn.Linear(hidden_size, latent_size))\n",
    "                self.decoder.add_module('decoder_layer{}'.format(i), nn.Linear(hidden_size, input_size))\n",
    "            else:\n",
    "                self.encoder.add_module('encoder_layer{}'.format(i), nn.Linear(hidden_size, hidden_size))\n",
    "                self.decoder.add_module('decoder_layer{}'.format(i), nn.Linear(hidden_size, hidden_size))\n",
    "                self.encoder.add_module('encoder_relu{}'.format(i), nn.ReLU())\n",
    "                self.decoder.add_module('decoder_relu{}'.format(i), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, num_layers):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.decoder = nn.Sequential()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.encoder.add_module('encoder_layer{}'.format(i), nn.Linear(input_size, hidden_size))\n",
    "                self.encoder.add_module('encoder_relu{}'.format(i), nn.ReLU())\n",
    "                self.decoder.add_module('decoder_layer{}'.format(i), nn.Linear(latent_size, hidden_size))\n",
    "                self.decoder.add_module('decoder_relu{}'.format(i), nn.ReLU())\n",
    "            elif i == num_layers - 1:\n",
    "                self.encoder.add_module('encoder_layer{}'.format(i), nn.Linear(hidden_size, latent_size))\n",
    "                self.decoder.add_module('decoder_layer{}'.format(i), nn.Linear(hidden_size, input_size))\n",
    "            else:\n",
    "                self.encoder.add_module('encoder_layer{}'.format(i), nn.Linear(hidden_size, hidden_size))\n",
    "                self.decoder.add_module('decoder_layer{}'.format(i), nn.Linear(hidden_size, hidden_size))\n",
    "                self.encoder.add_module('encoder_relu{}'.format(i), nn.ReLU())\n",
    "                self.decoder.add_module('decoder_relu{}'.format(i), nn.ReLU())\n",
    "        \n",
    "        self.mu = nn.Linear(latent_size, latent_size)\n",
    "        self.logvar = nn.Linear(latent_size, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        x = eps.mul(std).add_(mu)\n",
    "        x = self.decoder(x)\n",
    "        return x, mu, logvar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = torch.randn(100, 784)\n",
    "model = Autoencoder(input_size=784, hidden_size=400, latent_size=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(100):\n",
    "    X_hat = model(X)\n",
    "    loss = F.mse_loss(X_hat, X)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "X = torch.randn(100, 784)\n",
    "model = VAE(input_size=784, hidden_size=400, latent_size=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(100):\n",
    "    X_hat, mu, logvar = model(X)\n",
    "    loss = F.mse_loss(X_hat, X) + 0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1. - logvar)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "X = torch.randn(100, 784)\n",
    "Y = torch.randn(100, 784)\n",
    "model = GAN(input_size=784, hidden_size=400, latent_size=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(100):\n",
    "    X_hat, mu, logvar, D = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAN model for 1D time series data, with the goal of predicting the next time step\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GAN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.output_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        x = self.forward(x)\n",
    "        return F.mse_loss(x, y)\n",
    "    \n",
    "    def train(self, x, y, epochs=1000, lr=0.01):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch: {} Loss: {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/64 (0%)]\tLoss: 1.029464\n",
      "Train Epoch: 1 [0/64 (0%)]\tLoss: 1.021600\n",
      "Train Epoch: 2 [0/64 (0%)]\tLoss: 1.014361\n",
      "Train Epoch: 3 [0/64 (0%)]\tLoss: 1.007731\n",
      "Train Epoch: 4 [0/64 (0%)]\tLoss: 1.001698\n",
      "Train Epoch: 5 [0/64 (0%)]\tLoss: 0.996092\n",
      "Train Epoch: 6 [0/64 (0%)]\tLoss: 0.990910\n",
      "Train Epoch: 7 [0/64 (0%)]\tLoss: 0.986023\n",
      "Train Epoch: 8 [0/64 (0%)]\tLoss: 0.981444\n",
      "Train Epoch: 9 [0/64 (0%)]\tLoss: 0.977138\n",
      "Train Epoch: 0 [0/64 (0%)]\tLoss: 32.458233\n",
      "Train Epoch: 1 [0/64 (0%)]\tLoss: 28.404478\n",
      "Train Epoch: 2 [0/64 (0%)]\tLoss: 24.836149\n",
      "Train Epoch: 3 [0/64 (0%)]\tLoss: 21.695860\n",
      "Train Epoch: 4 [0/64 (0%)]\tLoss: 18.963074\n",
      "Train Epoch: 5 [0/64 (0%)]\tLoss: 16.590847\n",
      "Train Epoch: 6 [0/64 (0%)]\tLoss: 14.542610\n",
      "Train Epoch: 7 [0/64 (0%)]\tLoss: 12.766900\n",
      "Train Epoch: 8 [0/64 (0%)]\tLoss: 11.267582\n",
      "Train Epoch: 9 [0/64 (0%)]\tLoss: 9.981268\n"
     ]
    }
   ],
   "source": [
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.randn(64, 100) # 64 samples, 100 features\n",
    "y = torch.randn(64, 100) # 64 samples, 1 target\n",
    "train_dataset = torch.utils.data.TensorDataset(X, y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Autoencoder(input_size=100, hidden_size=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def loss_function(output, target, mu, logvar):\n",
    "    BCE = F.mse_loss(output, target)\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD     \n",
    "model = VAE(input_size=100, hidden_size=50, latent_size=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output, mu, logvar = model(data)\n",
    "\n",
    "        loss = loss_function(output, target, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
