{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 1d cnn model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN1d(nn.Module):\n",
    "    #create a 1d cnn regression model\n",
    "    #the ouput is a target value of 60 minutes later, is a scalar\n",
    "    def __init__(self, input_len, input_dim, kernel_size, layer_num, hidden_size,\n",
    "                  dropout=0.5, batch_norm=True):\n",
    "        #input shape: (batch_size, input_len, input_dim)\n",
    "        super(CNN1d, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.input_dim = input_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.layer_num = layer_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(self.input_dim, self.hidden_size, self.kernel_size, padding=self.kernel_size // 2)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(self.hidden_size)])\n",
    "        for i in range(self.layer_num - 1):\n",
    "            self.convs.append(nn.Conv1d(self.hidden_size, self.hidden_size, self.kernel_size, padding=self.kernel_size // 2))\n",
    "            self.bns.append(nn.BatchNorm1d(self.hidden_size))\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_len * self.hidden_size, 74)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        #initialize weights\n",
    "        for conv in self.convs:\n",
    "            torch.nn.init.xavier_uniform_(conv.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #forward pass\n",
    "        #input shape: (batch_size, input_len, input_dim)\n",
    "        #output shape: (batch_size, 1)\n",
    "        x = x.transpose(1, 2)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.batch_norm:\n",
    "                x = F.relu(self.bns[i](conv(x)))\n",
    "            else:\n",
    "                x = F.relu(conv(x))\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = x.view(-1, (self.input_len * self.hidden_size))\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        #predict the target value\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        #calculate loss\n",
    "        return F.mse_loss(self.forward(x), y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positional encoding\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    #add positional encoding to the input\n",
    "    def __init__(self, input_len, input_dim, dropout=0.5):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.input_dim = input_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pe = torch.zeros(self.input_len, self.input_dim)\n",
    "        position = torch.arange(0, self.input_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.input_dim, 2).float() * (-math.log(10000.0) / self.input_dim))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        self.pe.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        #forward pass\n",
    "        #input shape: (batch_size, input_len, input_dim)\n",
    "        #output shape: (batch_size, input_len, input_dim)\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    #create MULTIHEADATTENTION model\n",
    "    #the ouput is a target value of 60 minutes later, is a scalar\n",
    "    def __init__(self, input_len, input_dim, hidden_size, num_heads, layer_num, dropout=0.5):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_num = layer_num\n",
    "        self.dropout = dropout\n",
    "        #embedding\n",
    "\n",
    "        self.embedding = nn.Linear(self.input_dim, self.hidden_size)\n",
    "        self.q_linears = nn.ModuleList([nn.Linear(self.hidden_size, self.hidden_size) for i in range(self.layer_num)])\n",
    "        self.k_linears = nn.ModuleList([nn.Linear(self.hidden_size, self.hidden_size) for i in range(self.layer_num)])\n",
    "        self.v_linears = nn.ModuleList([nn.Linear(self.hidden_size, self.hidden_size) for i in range(self.layer_num)])\n",
    "        self.multihead_attns = nn.ModuleList([nn.MultiheadAttention(self.hidden_size, self.num_heads) for i in range(self.layer_num)])\n",
    "        self.norms = nn.ModuleList([nn.BatchNorm1d(self.input_len) for i in range(self.layer_num)])\n",
    "        self.out = nn.Linear(self.hidden_size * self.input_len, 74)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pe = torch.zeros(self.input_len, self.hidden_size)\n",
    "        position = torch.arange(0, self.input_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.hidden_size, 2).float() * (-math.log(10000.0) / self.hidden_size))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        self.pe.requires_grad = False\n",
    "        self.init_weights()\n",
    "        self.mean = nn.Parameter(torch.zeros(74))\n",
    "        self.std = nn.Parameter(torch.ones(74))\n",
    "\n",
    "    def init_weights(self):\n",
    "        #initialize weights\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.out.weight)\n",
    "        \n",
    "    def positional_encoding(self, x):\n",
    "        #use positional encoding\n",
    "        x = x + self.pe.to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #use positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        #forward pass\n",
    "        bs = x.size(0)\n",
    "        for i in range(self.layer_num):\n",
    "            q = self.q_linears[i](x)\n",
    "            k = self.k_linears[i](x)\n",
    "            v = self.v_linears[i](x)\n",
    "            x, _ = self.multihead_attns[i](q, k, v)\n",
    "            x = self.norms[i](x)\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(bs, -1)\n",
    "        x = self.out(x)\n",
    "        x = (x - self.mean) / self.std\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        #calculate loss\n",
    "        return F.mse_loss(self.forward(x), y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_samples = 1000\n",
    "#test the model\n",
    "X = torch.randn(num_samples, 4 * 74)\n",
    "Y = torch.randn(num_samples, 74)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "day = torch.randint(0, 30, (num_samples, 1))\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y, day, input_len=128):\n",
    "        #the input data is a 1d array, indicate the minute of the day\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.day = day\n",
    "        self.input_len = input_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #output previous self.input_len minutes data and target value\n",
    "        #if the there is no enough data in the same day, pad with 0\n",
    "        d = self.day[index]\n",
    "        start = index - self.input_len\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if self.day[start] != d:\n",
    "            while self.day[start] != d:\n",
    "                start += 1\n",
    "            #pad with 0 before start\n",
    "        if index - start < self.input_len:\n",
    "            x = torch.zeros(self.input_len, self.X.shape[1])\n",
    "            x[self.input_len - index + start: self.input_len] = self.X[start: index].clone()\n",
    "        else:\n",
    "            x = self.X[start: index]\n",
    "            \n",
    "        y = self.y[index].clone()\n",
    "        if x.shape != (self.input_len, self.X.shape[1]):\n",
    "            print(x.shape, index, start)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "mydataset = MyDataset(X, Y, day, input_len=120)\n",
    "train_index = range(0, int(num_samples * 0.8))\n",
    "test_index = range(0, int(num_samples * 0.8))\n",
    "#test_index = range(int(num_samples * 0.8), num_samples)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(mydataset, train_index)\n",
    "test_dataset = torch.utils.data.Subset(mydataset, test_index)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model with 4 GPUs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "model = Attention(input_len=120, input_dim=74 * 4, hidden_size=128, num_heads=4, layer_num=2, dropout=0.5)\n",
    "def correlation_t(x, y):\n",
    "    return torch.sum(x * y) / (torch.sqrt(torch.sum(x * x)) * torch.sqrt(torch.sum(y * y)))\n",
    "\n",
    "model = DataParallel(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    prediction = []\n",
    "    ground_truth = []\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        p = model(x)\n",
    "        loss = F.mse_loss(p, y)\n",
    "        prediction.append(p)\n",
    "        ground_truth.append(y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(epoch, i, loss.item())\n",
    "    prediction = torch.cat(prediction, dim=0)\n",
    "    ground_truth = torch.cat(ground_truth, dim=0)\n",
    "    print(prediction.shape, ground_truth.shape)\n",
    "    print(epoch, correlation_t(prediction, ground_truth).item())\n",
    "\n",
    "#the number of gpu \n",
    "print(torch.cuda.device_count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    #create MLP model\n",
    "    #the ouput is a target value of 60 minutes later, is a scalar\n",
    "    def __init__(self, input_len, input_dim, hidden_size, layer_num, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_num = layer_num\n",
    "        self.dropout = dropout\n",
    "        #embedding\n",
    "        self.embedding = nn.Linear(self.input_dim, self.hidden_size)\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.hidden_size, self.hidden_size) for i in range(self.layer_num)])\n",
    "        self.norms = nn.ModuleList([nn.BatchNorm1d(self.input_len) for i in range(self.layer_num)])\n",
    "        self.out = nn.Linear(self.hidden_size * self.input_len, 74)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "        self.mean = nn.Parameter(torch.zeros(74))\n",
    "        self.std = nn.Parameter(torch.ones(74))\n",
    "\n",
    "    def init_weights(self):\n",
    "        #initialize weights\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.out.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #forward pass\n",
    "        bs = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        for i in range(self.layer_num):\n",
    "            x = self.linears[i](x)\n",
    "            x = self.norms[i](x)\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(bs, -1)\n",
    "        x = self.out(x)\n",
    "        x = (x - self.mean) / self.std\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
